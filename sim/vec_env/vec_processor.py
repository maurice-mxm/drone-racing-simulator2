import numpy as np
import random
from sim.dynamics import drone_dynamics
from sim.dynamics import drone_dynamics2

"""
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
                                                                        vec_processor.py
                                                                        ----------------
                                        This is the Core of the Vec_Envs Initialisation. All of the "sub_envs" are 
                                        created and permanently updated by inputs of the PPO Algorithm. This is the 
                                        PROCESSOR CLASS of the Vec_Envs.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
"""


class VectorEnvironment:
    

    def __init__(self, config):

        """
        Initializes the VectorEnvironment class.
        Loads the configuration file and initializes the environment.

        Args:
            config (dict): Configuration for the given Vec_Env. Containing:
            - Number of Envs 
            - ...
        """

        # Extract number of Envs
        self.number_of_envs = config["nenvs"]    

        # Create environments
        self.envs = [BaseEnvironment() for _ in range(self.number_of_envs)]

        # Set observation and action dimensions
        self.obs_dim = self.envs[0].get_observation_dimension()
        self.act_dim = self.envs[0].get_action_dimension()

            


    def getObsDim(self):

        """
        Returns the dimension of the observation space.

        Returns:
            int: Dimension of the observation space.
        """

        return self.obs_dim
    
    
    def getActDim(self):

        """
        Returns the dimension of the action space.

        Returns:
            int: Dimension of the action space.
        """

        return self.act_dim
    
    
    def getNumEnvs(self):

        """
        Returns the number of environments.

        Returns:
            int: Number of environments.
        """

        return self.number_of_envs
    
    
    def reset(self, seed=0):

        """
        Resets all environments and returns the initial observations.

        Args:
            seed (int, optional): Random seed. Default is 0.

        Returns:
            np.ndarray: Initial observations of all environments.
        """

        observations_ = np.zeros((self.number_of_envs, self.obs_dim), dtype=np.float32)
        for i in range(self.number_of_envs):
            observations_[i] = self.envs[i].reset()

        return observations_

    def step(self, act):

        """
        Executes a step in all environments and returns the results (observations) generated by Dynamics.

        Args:
            actions (np.ndarray): Actions for all environments.

        Returns:
            tuple: Observations, rewards, done flags, and extra info.
        """

        observation_ = np.zeros((self.number_of_envs, self.obs_dim), dtype=np.float32)
        reward_ = np.zeros(self.number_of_envs, dtype=np.float32)
        done_ = np.zeros(self.number_of_envs, dtype=bool)
        
        for i in range(self.number_of_envs):
            observation_[i], reward_[i], done_[i]= self.step_env(i, act[i])

        return observation_, reward_, done_
    

    def step_env(self, env_id, action):

        """
        Executes a step for a single environment.

        Args:
            env_id (int): Environment ID.
            action (np.ndarray): Action for the environment.

        Returns:
            tuple: Observation, reward, done flag, and extra info.
        """

        observation, reward, done = self.envs[env_id].step(action)

        if done:
            observation = self.envs[env_id].reset()
            reward -= 10

        return observation, reward, done


    def get_observation(self, observations):

        """
        Retrieves current observations from all environments.

        Args:
            observations (np.ndarray): Array to store observations.
        """

        observations_ = np.zeros((self.number_of_envs, self.obs_dim), dtype=np.float32)
        for i in range(self.number_of_envs):
            observations_[i] = self.envs[i].get_observation()

        return observations_



class BaseEnvironment:

    def __init__(self):

        """
        Initializes the BaseEnvironment class. This is where the Dynamics are directly implemented.
        """

        self.initial_state = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])
        self.state = np.empty(12)

        for i in range(12):
            if i == 0:
                self.state[i] = random.uniform(-3, 3) 
            
            elif i == 1:
                self.state[i] = random.uniform(0, 3) 

            elif i == 2:
                self.state[i] = random.uniform(1.2, 1.4) 

            else:
                self.state[i] = random.uniform(-0.001, 0.001)
        #self.state = np.array([2.0, 0.0, 1.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
        
        
        self.obs_dim = 12
        self.act_dim = 4
        self.old_control = np.array([0.0, 0.0, 0.0, 0.0])
        self.initial_quat = drone_dynamics.euler_to_quaternion(self.state[6], self.state[7], self.state[8])
        self.quaternion = self.initial_quat.copy()
        


    def get_observation_dimension(self):

        """
        Returns the dimension of the observation space.

        Returns:
            int: Dimension of the observation space.
        """

        return self.obs_dim

    def get_action_dimension(self):

        """
        Returns the dimension of the action space.

        Returns:
            int: Dimension of the action space.
        """

        return self.act_dim

    def reset(self):

        """
        Resets the environment and returns the last State.

        Returns:
            np.ndarray: Sate of the environment.
        """
        self.state = np.empty(12)
        for i in range(12):
            if i == 0:

                self.state[i] = random.uniform(-3, 3) 

            elif i == 1:
                self.state[i] = random.uniform(0, 3) 

            elif i == 2:
                self.state[i] = random.uniform(1.2, 1.4) 

            else:
                self.state[i] = random.uniform(-0.001, 0.001)

        #self.state = np.array([2.0, 0.0, 1.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])


        self.quaternion = self.initial_quat.copy()

        return self.state

    def step(self, act):

        """
        Executes a step in the environment and updates the state.

        Args:
            action (np.ndarray): Action to be performed.

        Returns:
            tuple: New state and reward.

        """

        #print(self.state)

        self.state, reward, done = drone_dynamics2.dynamics(self.state, act, self.old_control)

        self.old_control = act.copy()

        return self.state, reward, done

    def is_terminal_state(self):

        """
        Checks if the environment is in a terminal state. Not used at the time given, directy implemented in dynamics (see step_env())

        Returns:
            bool: True if in a terminal state, False otherwise.
        """

        return self.state[0] > 3 or self.state[0] < -3 or self.state[1] > 3 or self.state[1] < -3 or self.state[2] > 6 or self.state[2] < 0.02


    def get_observation(self, observation):

        """
        Returns the current observation of the environment.

        Args:
            observation (np.ndarray): Array to store the observation.

        Returns:
            np.ndarray: The current state of the environment.
        """

        return self.state

